{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdnCtNSRK69//EvsOn916j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AXYUT-9/5CS037/blob/main/workshop5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EWN2IvJaDlvH"
      },
      "outputs": [],
      "source": [
        "#Define the cost function\n",
        "import numpy as np\n",
        "\n",
        "def cost_function(X, Y, W):\n",
        "\n",
        "  \"\"\" Parameters:\n",
        "  This function finds the Mean Square Error.\n",
        "  Input parameters:\n",
        "  X: Feature Matrix\n",
        "  Y: Target Matrix\n",
        "  W: Weight Matrix\n",
        "  Output Parameters:\n",
        "  cost: accumulated mean square error.\n",
        "  \"\"\"\n",
        "  # Your code here:\n",
        "  M=3\n",
        "  y_pred = np.dot(X,W)\n",
        "  cost = (1/2*M)*np.sum(np.square(y_pred-Y))\n",
        "  return cost\n",
        "\n",
        "\n",
        "  X_test = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "  Y_test = np.array([3, 7, 11])\n",
        "  W_test = np.array([1, 1])\n",
        "  cost = cost_function(X_test, Y_test, W_test)\n",
        "  if cost == 0:\n",
        "    print(\"Proceed Further\")\n",
        "  else:\n",
        "    print(\"something went wrong: Reimplement a cost function\")\n",
        "    print(\"Cost function output:\", cost_function(X_test, Y_test, W_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "Y_test = np.array([3, 7, 11])\n",
        "W_test = np.array([1, 1])\n",
        "cost = cost_function(X_test, Y_test, W_test)\n",
        "if cost == 0:\n",
        "  print(\"Proceed Further\")\n",
        "else:\n",
        " print(\"something went wrong: Reimplement a cost function\")\n",
        " print(\"Cost function output:\", cost_function(X_test, Y_test, W_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RHBCTqrGpS1",
        "outputId": "d596dc0f-4a01-48c2-fd7c-053b1595c98d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceed Further\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "\n",
        "  \"\"\"\n",
        "  Perform gradient descent to optimize the parameters of a linear regression model.\n",
        "  Parameters:\n",
        "  X (numpy.ndarray): Feature matrix (m x n).\n",
        "  Y (numpy.ndarray): Target vector (m x 1).\n",
        "  W (numpy.ndarray): Initial guess for parameters (n x 1).\n",
        "  alpha (float): Learning rate.\n",
        "  iterations (int): Number of iterations for gradient descent.\n",
        "  Returns:\n",
        "  tuple: A tuple containing the final optimized parameters (W_update) and the history of cost values\n",
        "  .\n",
        "  W_update (numpy.ndarray): Updated parameters (n x 1).\n",
        "  cost_history (list): History of cost values over iterations.\n",
        "  \"\"\"\n",
        " #Initialize cost history\n",
        "  cost_history = [0] * iterations\n",
        " #Number of samples\n",
        "  m = len(Y)\n",
        "  for iteration in range(iterations):\n",
        "\n",
        "# Step 1: Hypothesis Values\n",
        "    Y_pred = np.dot(X,W)\n",
        "\n",
        "# Step 2: Difference between Hypothesis and Actual Y\n",
        "    loss = Y_pred - Y\n",
        "# Step 3: Gradient Calculation\n",
        "    dw = (1/m)*np.dot(X.T,loss)\n",
        "# Step 4: Updating Values of W using Gradient\n",
        "    W_update = W - alpha * dw\n",
        "# Step 5: New Cost Value\n",
        "    cost = cost_function(X, Y, W_update)\n",
        "    cost_history[iteration] = cost\n",
        "  return W_update, cost_history"
      ],
      "metadata": {
        "id": "7b3rCmc0HrI4"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}